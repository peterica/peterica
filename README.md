
###  🐱 github stats  

<div id="main" align="center">
    <img src="https://github-readme-stats.vercel.app/api?username=peterica&count_private=true&show_icons=true&theme=radical"
        style="height: auto; margin-left: 20px; margin-right: 20px; padding: 10px;"/>
    <img src="https://github-readme-stats.vercel.app/api/top-langs/?username=peterica&layout=compact"   
        style="height: auto; margin-left: 20px; margin-right: 20px; padding: 10px;"/>
</div>

###  💁‍♀️ About Me  
<p align="center">
    <a href="https://peterica.tistory.com/"><img src="https://img.shields.io/badge/Blog-FF5722?style=flat-square&logo=Blogger&logoColor=white"/></a>
    <a href="mailto:ilovefran.ofm@gmail.com"><img src="https://img.shields.io/badge/Gmail-d14836?style=flat-square&logo=Gmail&logoColor=white&link=ilovefran.ofm@gmail.com"/></a>
</p>

<br>

### 📕 Latest Blog Posts   

<a href ="https://peterica.tistory.com/1001"> [AI] LLM 양자화 완전 정리: GPTQ / AWQ / GGUF / BNB 차이 + 비트 수와 메모리 관계 </a> <br>
<a href ="https://peterica.tistory.com/1000"> [AI] LLM 양자화와 벡터 양자화(VQ)의 차이 </a> <br>
<a href ="https://peterica.tistory.com/999"> [AI] Transformer: Attention Is All You Need 논문 요약 </a> <br>
<a href ="https://peterica.tistory.com/998"> [AI] Transformer, BERT, GPT 한 번에 정리하기 </a> <br>
<a href ="https://peterica.tistory.com/997"> [AI] RAG&middot;Agent&middot;Infra&middot;Compliance를 동시에 다루는 엔지니어의 지식 정리법 </a> <br>
<a href ="https://peterica.tistory.com/994"> 백엔드 개발자가 정리한 PIM vs PNM: 메모리 근처에서 연산한다는 것의 진짜 의미 </a> <br>
